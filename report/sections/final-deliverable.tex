\section{Final BNF}\label{sec:final-bnf}

\input{sections/final-bnf}

\section{Final GUI}\label{sec:final-gui}

The final GUI is shown in subsections~\ref{subsec:code-editor},~\ref{subsec:notebook-view} and~\ref{subsec:plot-view}.
It has 3 types of windows: the code editor, the notebook view and the plot view.
It has support for loading in or saving to a file, buttons for running or transpiling the code, a button for parse 
tree visualisation and an interactive repl.
The notebook view supports importing or exporting from a proprietary file format, running code blocks and exporting 
to a PDF\@.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/finalCodeEditor}
    \includegraphics[width=0.8\textwidth]{assets/finalGuiParserError}
    \includegraphics[width=0.8\textwidth]{assets/finalGuiTypeError}
    \includegraphics[width=0.8\textwidth]{assets/finalGuiParseTree}
    \includegraphics[width=0.8\textwidth]{assets/finalREPL}
    \caption{Final GUI}\label{fig:final-gui}
\end{figure}

\subsection{Code Editor}\label{subsec:code-editor}

The code editor is where the user writes their code, and is the main window of the GUI\@.
It has syntax highlighting, live error checking and a REPL\@.
Files can be loaded into the editor, and the code can be saved to a file.
The user can run the code or transpile to C by pressing the appropriate button, as well as visualise the parse tree.

Images are given in figure~\ref{fig:final-gui} of the code editor with output, live parser errors and live type errors.

\subsection{Notebook View}\label{subsec:notebook-view}

The notebook view is where the user can write code in a more interactive way, similar to Jupyter notebooks\citep{Jupyter}.
It has support for importing or exporting from a proprietary file format, running code blocks and exporting to a PDF\@.
An image is shown in figure~\ref{fig:notebook-view}.
As shown, variable assignment persists between code blocks, the result of the code block is displayed below the code
and plotting functions are supported.

\begin{figure}[H]
    \centering 
    \includegraphics[width=0.8\textwidth]{assets/finalNotebook}
    \caption{Notebook view}\label{fig:notebook-view}
\end{figure}

An example PDF export from the given image is shown in figure~\ref{fig:pdf-export}.

\begin{figure}[H]
    \centering
    \includegraphics[page=1,width=0.8\textwidth]{assets/plotPDF}
    \includegraphics[page=2,width=0.8\textwidth]{assets/plotPDF}
    \caption{PDF export}\label{fig:pdf-export}
\end{figure}

\subsection{Plot View}\label{subsec:plot-view}

The plot view is where the user can view plots generated by the code.
It has support for zooming in and out, moving around and adjusting the axes, as well as interactive input for
plotting functions.
Images and further details are given in section~\ref{sec:plotting}.

\subsection{REPL}\label{subsec:repl}

The main view also includes a REPL input at the bottom, allowing the user to interact with the program in a more
interactive way.
After running the code, the user can type expressions in the REPL and have them evaluated, with the result being
displayed on the right.
Note that the results don't have to be explicitly printed, as the REPL will display the result of the last expression
evaluated.
An image is shown in figure~\ref{fig:final-gui}.

\subsection{Syntax Window}\label{subsec:syntax-window}

A window containing examples of the syntax of the language is also included, allowing the user to quickly see how to
write certain expressions or functions.
This is accessed by typing \textit{help()} in the REPL\@.

\section{Notable Features}\label{sec:notable-features}

\input{sections/notable-features}

\section{Lexer}\label{sec:lexer}

Initial lexer design was based on a simple regular expression based lexer, but this was later replaced with a more
functional approach using pattern matching on the input string.
The reason for this change was that the regular expression based lexer was difficult to extend and maintain due to 
the lack of type safety.
For example if we had a more general regex called before a more specific one, the more general one would always match
first, even if the more specific one was intended.

This was solved by using a more functional approach, where the type system of F\# would inform us if a case would 
never be matched due to the order of the cases or otherwise, preventing a class of easily overlooked errors during development.
The lexer is now implemented as a recursive pattern matching function that takes a string and returns a list of 
tokens, complete with their lexeme and position in the input string.

Lexer errors are also accumulated in a list of type \textit{LexerError}, which are displayed to the user in the GUI\@.

Something of note is that the lexer parses numbers itself, rather than passing them to the parser as strings.

Additionally, due to the permittance of user defined operators, the lexer makes special considerations when lexing 
special characters, as the distinction between a built-in operator (with precedence) and a user defined operator (
currently without taking precedence into account) is made during lexing.

Furthermore, both block comments (\textit{/* */}) and line comments (\textit{//}) are handled by the lexer by ignoring
the contents of the comment.
In future, it may be interesting represent comments as a token in the AST, allowing for systems such as documentation
generation or automatic formatting to be implemented.

\section{Parser}\label{sec:parser}

The parser is implemented using Pratt parsing\citep{pratt1973top}, which is a top-down operator precedence parsing 
method that allows for easy extension and modification of the grammar.
It works by assigning a precedence to each token, as well as functions specifying how to parse the token when 
encountering it in a prefix, infix or postfix position.

For example, take the expression $2 + 3 * 4$.

The parser would first encounter the number \textit{2}, which has a precedence of 0 and a prefix function that
simply returns the number.
Thus, the current state of the parser is $2$.
The parser would then encounter the operator \textit{+}, which has a precedence of 1 and a left associative infix
function that takes the left hand side and the right hand side and returns a binary expression node.
The parser then attempts to parse the right hand side of the operator with a precendence level higher than the
plus operator, as Pratt parsing must ensure that higher precedence operations (such as multiplication) are parsed
first.
The parser would then encounter the number \textit{3}, which again is treated as a literal and returned.
The parser then encounters the operator \textit{*}, which has a precedence of 2 (higher than the plus operator) and 
as such the parser cannot yet resolve the \textit{+} operator; it must handle the higher precedence multiplication
operator first.
The parser saves the left hand side (the number 3) and then parses the right hand side of the multiplication 
operator using a precedence level higher than the multiplication operator.
It encounters the number \textit{4}, which is returned as a literal.
The parser then returns the binary expression node for the multiplication operator, with the left hand side being
the number 3 and the right hand side being the number 4.
The parser then returns to the plus operator, which can now be resolved as the left hand side is the number 2 and the
right hand side is the result of the multiplication operator.

This is a simple example, but Pratt parsing can handle more complex expressions with ease, such as nested
expressions and function calls.

Using Pratt parsing has improved the extensibility of the parser, as adding new operators or changing the grammar
is as simple as adding a new case to the parser.

A slight limitation is during ambiguity, such as the \textit{(} symbol, which can be used for a grouping, a lambda 
definition, a tuple or a \textit{unit} type when encountered in the prefix position.
This is resolved through a state machine approach, where the parser can move around the state at will, allowing 
lookahead and backtracking in order to reach a point where the ambiguity is resolved.

In order to simplify the Virtual Machine\ref{sec:virtual-machine}, the parser parses all binary and unary operations 
as function calls, with the operator as the function name.

In order to make type inference simpler for operators that are overloaded for both unary and binary operations (such 
as the \textit{-} operator), the operator itself keeps track of the manner in which it is called (unary or binary) and
returns the appropriate AST node. 
This allows for easier type inference (as the names of the overloaded functions are different), 
and simplifies the bytecode generation process by removing ambiguity in the AST\@.
This idea could possibly be extended to allow other overloaded function names (with varying numbers of arguments or 
arguments of different types).

Error handling in the parser is done through the use of the monadic \textit{ParserResult} type, which can either 
return a successful result or an error message, which is then displayed to the user in the GUI allowing for clear 
and easy diagnosis of errors.

\section{AST}\label{sec:expression}

The AST of the language is represented as a list of statements, where a statement is either an expression, a
variable assignment or another statement type (such as a recursive function definition).
It is typed (after type inference\ref{sec:type-inference}) in order to allow for easier optimisation and
bytecode generation.

The AST representation is given in section~\ref{sec:final-bnf}.

\section{Type Inference}\label{sec:type-inference}
\input{sections/type-inference}

\section{Optimisation}\label{sec:optimisation}

Before compilation, the AST is optimised by removing dead code and constant folding.

\subsection{Dead code elimination}\label{subsec:dead-code-elimination}

Dead code elimination is performed on the AST by removing any statements that are not used.
For example, if a variable is declared but never used, the variable declaration is removed or if an expression is
written but never used, the expression is removed.

This is achieved using a simple algorithm that traverses the AST and removes any nodes that are not used.

\begin{algorithmic}
    \While{Dead code can be removed}
        \For{Each node in the AST}
            \If{Node is a statement}
                \If{Statement is not used}
                    \State Remove statement
                \EndIf
            \ElsIf{Node is an expression}
                \If{Expression is not used}
                    \State Remove expression
                \EndIf
            \ElsIf{Node is a binding}
                \If{Variable is not used}
                    \State Remove binding
                \EndIf
            \EndIf
        \EndFor
    \EndWhile
\end{algorithmic}

The process is repeated until no more dead code can be removed, allowing for long chains of dead code to be 
removed (for example if a variable is used in a function that is never called, the function would first be removed
and then the variable).
It is to be noted that variable assignments are never removed during DCE when running the code editor due to the 
attached REPL, as the user may wish to use the variable in the REPL\@, or when running code blocks in the notebook 
view\ref{subsec:notebook-view} as the variable may be used in a later code block.
However, DCE can be aggressively performed when transpiling to C\ref{sec:transpiler-implementation}, as the user is not expected to 
interact with the generated C code.

\subsection{Constant folding}\label{subsec:constant-folding}

Constant folding is performed on the AST by evaluating constant expressions at compile time, such as $2 + 2 \ra 4$.
This is accomplished using the initial interpreter implementation, which recursively evaluates the AST and replaces
constant expressions with their evaluated value.
Only constants are evaluated, and thus no variable resolution is performed due to the cost of this operation.

\section{Bytecode Virtual Machine and Compiler}\label{sec:initial-design-of-the-bytecode-virtual-machine-and-compiler}

\input{sections/initial-design-of-the-bytecode-virtual-machine-and-compiler}

\section{Prelude}\label{sec:prelude}

A prelude is implicitly imported into every program, which contains some useful functions defined in the language, as 
well as wrappers for the built-in functions of the Virtual Machine, such as \textit{cos}, \textit{log}, etc.

Notable functions include:

\begin{itemize}
    \item \textit{map}, \textit{fold} and \textit{filter} functions for lists.
    \item \textit{range} function for generating a list of numbers.
    \item \textit{sqrt}, \textit{cubeRoot} which are specialisations of the \textit{root} function.
    \item \textit{head}, \textit{tail} and \textit{len} functions for lists.
    \item \textit{findIntegral} function for finding the integral of a function.
\end{itemize}

We felt it was useful implementing these in-language functions as it allows for more concise and readable code, as
well as showcasing the power of the language.

\section{Plotting}\label{sec:plotting}

\input{sections/plotting}

\section{Drawing}\label{sec:drawing}

As well as plotting, the user also has the option of drawing arbitrary shapes on a canvas, and attaching event 
listeners to them.
This is done by means of the \textit{draw} function, which takes in a record of configuration options of the following
type:

\begin{minted}{fsharp}
    type DrawConfig = {
        x: float,
        y: float,
        width: float,
        height: float,
        color: string,
        shape: "rectangle" | "circle",
        trace?: bool, 
    }
\end{minted}

Or a list of the above record type, allowing for multiple shapes to be drawn on the same canvas.

For example, the following will draw a few circles on the canvas:

\begin{minted}{fsharp}
let circle1 = {
    x = 100.0,
    y = 100.0,
    width = 50.0,
    height = 50.0,
    colour = "red",
    trace = true
}
let circle2 = {
    x = 200.0,
    y = 200.0,
    width = 50.0,
    height = 50.0,
    colour = "blue",
    trace = true
}
draw([circle1, circle2])
\end{minted}

The result of the above code is shown in image~\ref{fig:draw-circles}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/drawImage}
    \caption{Draw circles}\label{fig:draw-circles}
\end{figure}

The \textit{draw} function then returns a unique identifier for the shape, which can be used to attach event 
listeners, allowing for movement of the shape through key presses.

The following example attached event listeners to a shape which moves it left and right following the \textit{cos} curve:

\begin{minted}{fsharp}
on(id, Keys.Right, (state) -> { x = state.x + 10.0, y = cos(state.x) * 10.0 + 100.0 })
on(id, Keys.Left, (state) -> { x = state.x - 10.0, y = cos(state.x) * 10.0 + 100.0 })
\end{minted}

Where keys is a record defined in the prelude of the language (see~\autoref{sec:prelude}).

Additionally, the \textit{trace} option allows for the shape to leave a trail behind it, which can be useful for
animations or visualising movement.

\section{Transpiler Implementation} \label{sec:transpiler-implementation}
The Vec3 transpiler provides an alternative execution path by converting Vec3 programs into C code, enabling direct compilation to native machine code.
This approach offers potential performance benefits while maintaining the language's semantics through careful mapping of Vec3's features to equivalent C constructs.

\subsection{Architecture Overview}\label{subsec:architecture-overview}
The transpiler follows a multiphase pipeline that transforms Vec3 source code into executable programs.
The process begins with the existing parser and type checker, then proceeds through code generation and compilation phases.
Each phase is designed to be independent, allowing for easier maintenance and optimisation of individual components.
The transpilation process consists of the following main phases:

\begin{itemize}[nolistsep]
\item Source code parsing and AST generation
\item Type checking and semantic analysis
\item C code generation
\item Compilation of generated C code to native executables
\end{itemize}

\subsection{Code Generation Strategy}\label{subsec:code-generation-strategy}
The code generator maps Vec3's high-level constructs to equivalent C implementations while preserving the language's semantics. 
This mapping is particularly careful in handling Vec3's functional features, which don't have direct equivalents in C. The generator maintains a reference counting system for memory management and implements a runtime environment that supports Vec3's dynamic features.
The transpiler generates C code that relies on a custom runtime library implementing Vec3's core features:

\begin{itemize}[nolistsep]
\item Complex number arithmetic
\item First-class functions
\item Garbage collection through reference counting
\item Dynamic typing with runtime type checking
\item List and tuple operations
\item Record types with dynamic field access
\end{itemize}

\section{Memory Management and Garbage Collection}
\label{sec:memory-management}

The Vec3 runtime implements a deterministic memory management system through reference counting\citet{christopher1984reference}, offering 
predictable cleanup behavior while avoiding the complexity of tracing garbage collection.
This approach particularly suits Vec3's transpiled nature, as it maps cleanly to C's manual memory management while providing automated cleanup semantics.

\subsection{Reference Counting Implementation}\label{subsec:reference-counting-implementation}
The reference counting system centers around the Vec3Object structure, which serves as the base for all Vec3 values:

\begin{minted}{c}
typedef struct Vec3Object {
    Type type;
    int ref_count;
    void (destructor)(struct Vec3Object);
} Vec3Object;
\end{minted}

Each object tracks its reference count, and the runtime provides two core operations:

\begin{minted}{c}
void vec3_incref(Vec3Value* value) {
    if (value != NULL) {
    value->object.ref_count++;
}

void vec3_decref(Vec3Value* value) {
    if (value != NULL) {
        assert(value->object.ref_count > 0);
        value->object.ref_count--;
            if (value->object.ref_count == 0) {
                if (value->object.destructor) {
                    value->object.destructor((Vec3Object*)value);
            }
            free(value);
        }
    }
}
\end{minted}

\subsection{Destructor System}\label{subsec:destructor-system}
The destructor function pointer in Vec3Object enables type-specific cleanup.
Each value type can register its own destructor:

\begin{itemize}[nolistsep]
\item Strings need to free their character buffer
\item Lists must decrement references to their elements
\item Functions must clean up their environment and name
\item Records must deallocate their hash table and decrement field references
\end{itemize}

This extensible destructor system ensures proper cleanup of complex data structures:

\begin{minted}{c}
void vec3_destroy_list(Vec3Object* object) {
    Vec3Value* value = (Vec3Value*)object;
    vec3_list* current = value->as.list;
    while (current != NULL) {
        vec3_list* next = current->next;
        vec3_decref(current->value);
        free(current);
        current = next;
    }
}
\end{minted}

\section{Generic Value Representation}\label{sec:generic-value}

Vec3 implements a unified value representation.
This design allows Vec3 to support multiple data types within a single value structure while providing type-safe operations.

\subsection{Value Structure}\label{subsec:value-structure}

The Vec3Value structure combines a type tag with a union of possible values:

\begin{minted}{c}
typedef struct {
    Vec3Object object;
    union {
        Number number;
        struct {
            size_t length;
            char* chars;
        } string;
        vec3_list* list;
        bool boolean;
        Vec3Function* function;
        HashMap* record;
    } as;
} Vec3Value;
\end{minted}

This design achieves several key objectives:

\begin{itemize}[nolistsep]
    \item Efficient memory usage through union-based storage
    \item Fast type checking via the object type tag
    \item Uniform handling of reference counting
    \item Support for value-specific destructors
\end{itemize}

\subsection{Number System}\label{subsec:number-system}

Vec3's number system demonstrates the flexibility of this approach by supporting multiple numeric types within a single representation:

\begin{minted}{c}
typedef struct {
    NumberType type;
    union {
        int64_t integer;
        double float_val;
        struct { int64_t num; int64_t denom; } rational;
        struct { double real; double imag; } complex;
    } as;
} Number;
\end{minted}

This nested union structure allows Vec3 to efficiently represent different numeric types while maintaining type safety through the NumberType tag.

\subsection{Type-Safe Operations}\label{subsec:type-safe-operations}

The value system includes comprehensive type checking and conversion operations.
For example, the truthiness check demonstrates type-specific behavior:

\begin{minted}{c}
bool vec3_is_truthy(const Vec3Value* value) {
    if (value == NULL || value->object.type == TYPE_NIL) {
        return false;
    }
    switch (value->object.type) {
        case TYPE_NUMBER:
            return value->as.number.type != NUMBER_INT || value->as.number.as.integer != 0;
        case TYPE_STRING:
            return value->as.string.length > 0;
        case TYPE_LIST:
            return value->as.list != NULL;
        case TYPE_BOOLEAN:
            return value->as.boolean;
        default:
            return true;
    }
}
\end{minted}

\subsection{Function Translation}\label{subsec:function-translation}

One of the more complex aspects of the transpiler is the translation of Vec3's first-class functions to C\@.
The transpiler generates wrapper structures that capture the function's environment and parameters:

\begin{minted}{c}
typedef struct {
    char* name;
    int arity;
    Vec3Value* (fn)(Vec3Value* args);
    struct Vec3Env* env;
    bool is_recursive;
} Vec3Function;
\end{minted}

This structure enables proper handling of closures and maintains Vec3's lexical scoping rules in the generated C code. 
The transpiler pays special attention to recursive functions, implementing them through a trampolining mechanism to prevent stack overflow in tail-recursive cases.

\section{Plotting}\label{sec:plotting-transpiler}
The plotting system in the Vec3 transpiler is built on top of GnuPlot\citet{gnuPlot}, providing a powerful and flexible 
interface for data visualisation.
GnuPlot was chosen for its robust plotting capabilities, wide range of plot types, and ability to generate 
high-quality visualisations.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/gnuplot}
    \caption{Example GnuPlot output showing two parabolas}\label{fig:gnuplot-example}
\end{figure}

Figure~\ref{fig:gnuplot-example} demonstrates the output of \textit{plotFuncs}~\ref{sec:plotting} with two parabolic functions:

\begin{minted}{fsharp}
let f = (x) -> x^2.0 - 2.0     // Upward parabola shifted down
let d = (x) -> -x^2.0 + 2.0    // Downward parabola shifted up
plotFuncs("Functions", [f, d])
\end{minted}

The purple line shows $f(x) = x^2 - 2$, while the green line shows $d(x) = -x^2 + 2$.
This example demonstrates the system's ability to handle multiple functions and generate clear, distinguishable plots.

\subsection{Error Handling}\label{subsec:error-handling}

The transpiler implements comprehensive error handling throughout the generation process. 
It categorises errors into the distinct types \textit{CodeGenError} and \textit{CompilationError}, allowing for
granular error reporting and easy identification of issues.

\subsection{Build System Integration}\label{subsec:build-system-integration}
The transpiler includes a configurable build system that manages the compilation pipeline.
It handles directory structure creation, runtime library inclusion, and compiler invocation.
The system is designed to be platform-independent, automatically adjusting paths and compiler commands based on the operating system:

\begin{minted}{fsharp}
type TranspilerConfig =
    { OutputDir: string
      RuntimeDir: string
      IncludeDir: string
      CompilerPath: string }
\end{minted}

This configuration system ensures consistent behavior across different development environments while maintaining flexibility for different deployment scenarios.

\subsection{Performance Considerations}\label{subsec:performance-considerations}

The transpiler's output prioritises runtime performance through several optimisation strategies:

\begin{itemize}[nolistsep]
\item Minimises dynamic memory allocations where possible
\item Implements efficient reference counting mechanisms
\item Generates code that can benefit from the C compiler's optimisations
\item Structures code to enable the C compiler's full range of optimisations
\item Produces code patterns that potentially outperform interpreted execution
\end{itemize}

\section{Future Developments}\label{sec:future-developments}
While the current transpiler implementation successfully handles the core Vec3 language features, several areas present opportunities for future enhancement:

\begin{itemize}[nolistsep]
\item Optimisation of generated code patterns
\item Improved debug information generation
\item Enhanced cross-platform compatibility
\item Expanded runtime library capabilities
\end{itemize}

The modular design of the transpiler makes it well-suited for such future expansions while maintaining its current reliability and functionality.

\section{Code architecture}\label{sec:code-architecture}
The solution is split into 3 separate projects: the GUI, Tests and the main interpreter / compiler project.

\paragraph{The GUI} is where the project runs from, and is a standard Avalonia project, with a main window (the code editor), 
and then some other windows for the plots, drawing and notebook view, as well as some non-UI files such as a helper 
module for PDF exports.
It also contains a directory containing the prelude and standard library of the language, which are then imported as 
desired (except the prelude, which is always implicitly imported).
The frameworks are libraries used in the GUI project include AvaloniaUI, AvaloniaEdit, FSharp.Core, ScottPlot, 
TextMate and QuestPDF\@. 

\paragraph{The Tests} project mirrors the main project, with a similar structure and file naming for clear and sensible
organisation.
It uses the NUnit framework for testing, and tests the lexer, parser, compiler and type inference.

\paragraph{The main project} is where the main interpreter and compiler are implemented, and is split into the following
submodules:
\begin{itemize}
    \item \textit{Syntax Analysis} contains the lexer and parser, as well as the AST of the language.
    \item \textit{Type Analysis} contains the type inference engine.
    \item \textit{Backend} contains the compiler and virtual machine.
    \item \textit{Optimisation} contains the optimisation passes (dead code elimination and constant folding).
    \item \textit{Transpiler} contains the transpiler and runtime library.
\end{itemize}
A couple more files, such as the REPL, which contains helper functions for compilation and running of code, and the
Symbolic Expression modules, which contain helper functions for maths operations such as simplification of formulas 
and calculation of derivatives, are also present.
No external libraries are used in the main project, as it is a standalone F\# project.

The \textit{Executable} directory is also present in the root of the project, which contains the transpiler's runtime.

We felt this structure was sensible, as it allowed for clear separation of concerns and easy navigation of the codebase.

