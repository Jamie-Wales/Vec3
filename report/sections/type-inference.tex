Vec3 is a statically typed language, with full type inference.
The type inference algorithm is based on Hindley-Milner type inference\citep{sulzmann2000general}, with some
modification to support the non-ML style syntax, and extended to support \textit{row polymorphism}\citep{morris2019abstracting} (\ref{subsec:row-polymorphism}), \textit{gradual typing}\citep{garcia2016abstracting}
(\ref{subsec:gradual-typing}), \textit{recursive bindings} (\ref{subsec:recursive-bindings}), 
\textit{vector length encoding} (\ref{subsec:vector-length-enc}) and a seemingly unique method of supporting ad-hoc 
polymorphism named \textit{constraints} (\ref{subsec:constraints}).

The reason for implementing strong type inference due to the \textit{Semantic Soundness Theory}\citep{timany2024logical}, which states that a \textit{well-typed program cannot go wrong}.

Of course this is not strictly true in practice due to external factors, but it is certainly true that strong typing
rules out a large class of errors, most of which human, and as such it is a valuable tool for a maths language to
have as the user is less likely to make trivial mistakes.

Another thing to note is that in order to make the language more intuitive to use, the \textbf{integer} type will 
coerce into any other number type (\textbf{float, rational, complex}). 
This allows for expressions such as $5.0 ^ 5$ type checking successfully, with the result being a float.

\subsection{Type Inference Algorithm}\label{subsec:type-inference-algorithm}

The algorithm used to infer types is based on Algorithm W\citep{milner1978theory}.
The general idea is to assign the widest type possible for a given node in the AST, which is generally a
\textit{type variable}, which is a type used to represent a type that can be unified with any other type (a generic
type).
The node's children are then inferred, and the types of the children are unified with the parent node.
If the types cannot be unified, then the program is ill-typed.
Unification is the process of finding the most general type that can be assigned to two types, and is a key part of
the algorithm.

For example, unifying \textit{int} and \textit{int} would result in \textit{int}, as this is the most general type that
can be assigned to both.

Contrasting this, unifying \text{int} and a type variable \textit{a}, would result in \textit{int}, and then the type
variable \textit{a} would have to be substituted with \textit{int} throughout the program (because \textit{int} is the
most general type that can be assigned to \textit{a}).
It works bottom-up as only a few types are known at the start, such as the types of literals and the types of
built-in functions.

\paragraph{Algorithm Implementation}\label{par:algorithm-implementation}
A simplified version of the algorithm, with some details omitted for brevity, is shown in Algorithm~\ref{alg:algorithm}.

\begin{algorithm}
    \caption{Type Inference Algorithm}
    \begin{algorithmic}[1]
        \Function{$unify$}{$type1,\ type2$}
            \If{$type1\ \textbf{is}\ type\ variable$}
                \State $type1 \gets type2$
            \EndIf
            \If{$type2\ \textbf{is}\ type\ variable$}
                \State $type2 \gets type1$
            \EndIf
            \If{$type1\ \textbf{is}\ function\ type\ and\ type2\ \textbf{is}\ function\ type$}
                \State $unify\ paramTypes$
                \State $unify\ returnTypes$
            \EndIf
            \If{$type1$\ \textbf{is}\ not\ equal\ to\ $type2$}
                \State \textbf{error}
            \EndIf
        \EndFunction

        \Function{$infer$}{$expr,\ env$}
            \If{$expr\ \textbf{is}\ literal$}
                \State \Return $type\ of\ literal$
            \EndIf
            \If{$expr\ \textbf{is}\ variable$}
                \State $T \gets lookup\ variable\ env$
                \State \Return $type$
            \EndIf
            \If{$expr\ \textbf{is}\ function\ call$}
                \State $funcType \gets infer\ function$
                \State $argTypes \gets infer\ arguments$
                \State $funcType \gets unify\ paramTypes\ argTypes$
                \State $returnType \gets return\ type\ of\ funcType$
                \State \Return $returnType$
            \EndIf
            \If{$expr\ \textbf{is}\ binding$}
                \State $bodyType \gets infer\ body$
                \State $env \gets add\ binding\ bodyType\ environment$
                \State \Return $bodyType$
            \EndIf
            \If{$expr\ \textbf{is}\ lambda$}
                \State $argTypes \gets new\ type\ variables$
                \State $bodyEnv \gets add\ arguments\ to\ environment$
                \State $bodyType \gets infer\ body\ with\ bodyEnv$
                \State $funcType \gets argTypes+bodyType$
                \State \Return $funcType$
            \EndIf
        \EndFunction
    \end{algorithmic}\label{alg:algorithm}
\end{algorithm}

As shown, it is an incredibly simple yet powerful algorithm, and is the basis for many modern type inference
algorithms, such as that of F\# and OCaml.

\paragraph{Bindings}\label{par:bindings}
Generally in implementations of \textit{Algorithm W}, after type inference for a given binding has taken place a
process known as \textit{generalisation} occurs.
This is the process of replacing all type variables in the type of
the binding with \textit{forall} quantifiers, which is a way of saying that the type is polymorphic and can therefore be
instantiated with any type.

However, this was not necessary in our implementation as we don't specialise bindings during the instantiation of
types (such as during calls), we simply infer the type of the call and check it against the type of the binding, so
generalisation is not necessary.

\subsection{Gradual Typing}\label{subsec:gradual-typing}

Gradual typing is a type system that allows for the gradual transition from dynamic typing to static typing.
This is useful in a language like Vec3 as it allows for the user to write code without having to worry about types
allowing for quick prototyping, but then add types later to ensure correctness.
Users have the option of adding types to their code in the form $let\ x: int = 5$, and the type inference algorithm
will check that the type of the expression matches the type given.

The type $any$ can also be used, which represents a dynamic type that can be unified with any other type.
This disables the safety guarantees of the type system, but can be useful as mentioned above for quick prototyping.
One thing to note however is that the $any$ type is infectious, meaning that if a type is inferred to be $any$ then
the type of the parent node will also be $any$.

\subsection{Row Polymorphism}\label{subsec:row-polymorphism}

Row polymorphism is a form of polymorphism that allows for the definition of functions that operate on records with
a certain set of fields, but can also operate on records with additional fields.
It can be considered both a form of structural typing like that of TypeScript\citep{bierman2014understanding}, and a
form of subtyping.

For example, consider the following function:

\begin{minted}{fsharp}
let f = (x) -> x.a
\end{minted}

This function takes a record with a field \textit{a} and returns the value of that field.

Now consider the following record:

\begin{minted}{fsharp}
let r = {a = 5, b: int = 6}
\end{minted}

The function \textit{f} can be called with \textit{r} as an argument as \textit{r} has a field \textit{a}, and the
function will return 5.
This is a powerful feature as it allows for the definition of functions that operate on a wide range of records.
The reference algorithm given by \citet{morris2019abstracting} was used as a basis for the implementation of row
polymorphism in Vec3, however without record restriction as it was not necessary for the language.

The algorithm works by assigning a \textit{row variable} to each record type, which is a type variable that represents
the fields of the record, where a record is represented in the type system as an extension of another record, or the empty record.
The algorithm then unifies the row variables of the record types, and if the unification is successful then the
records are considered to be the same type.

A reference implementation of the algorithm is given in Algorithm~\ref{alg:row-polymorphism}.

\begin{algorithm}
    \caption{Row Polymorphism Algorithm}
    \begin{algorithmic}
        \Function{$unify$}{$type1,\ type2$}
            \If{$type1\ \textbf{is}\ row\ variable$}
                \State $type1 \gets type2$
            \EndIf
            \If{$type2\ \textbf{is}\ row\ variable$}
                \State $type2 \gets type1$
            \EndIf
            \If{$type1\ \textbf{is}\ record\ type\ and\ type2\ \textbf{is}\ record\ type$}
                \State $unify\ row\ variables$
            \EndIf
            \If{$type1$\ \textbf{is}\ not\ equal\ to\ $type2$}
                \State \textbf{error}
            \EndIf
        \EndFunction
    \end{algorithmic}
    \label{alg:row-polymorphism}
\end{algorithm}

With some creativity, row polymorphism can be used to represent semi-algebraic data types or tagged unions.
For example, consider the built-in \textit{on} function (used to add event listeners for shapes)\ref{sec:drawing}:

\begin{minted}{fsharp}
on(shape, Keys.Down, (state) -> ...)
\end{minted}

The function expects a shape reference, a key, and a function that takes a state.

The implementation of the keys record is hidden from the user, but could well be implemented as a record with a field
for each key, where each key is a record that contains a field 
\textit{43hr4h54j3} (a unique identifer for the keys record) and the \textit{on} function could have a type of:
\begin{minted}{fsharp}
let on = (shape, key: { 43hr4h54j3: int }, func) -> ...
\end{minted}

This has pretty good type safety, as the function will only accept keys with said field, which is hidden from the user.
This doesn't have quite as good safety guarantees as a true algebraic data type, i.e.\ in the form 
$data\ bool\ =\ True\ |\ False$, but is certainly safer than say C enums, preventing mistakes such as using incorrect 
argument order.

\subsection{Recursive Bindings}\label{subsec:recursive-bindings}

Due to the fact that everything is immutable in Vec3, the simplest way to ensure Turing completeness is to allow for 
recursive bindings (i.e.\ functions that call themselves).
This is a powerful feature as it allows for the definition of functions that operate on recursive data structures, such
as trees and lists.

The type inference algorithm was modified to support recursive bindings, as the standard algorithm would not be able to
infer the type of a recursive due to the fact that the binding would not be in the environment when the type of the
function was inferred (all functions are lambdas, and therefore are not assigned to a binding until after declaration).
Hence, recursive functions were introduced as a separate statement in the grammar of the language, and the type
inference algorithm was modified to support them by adding the binding to the environment before inferring the type of
the function.

\subsection{Constraints}\label{subsec:constraints}

Due to the restrictiveness of the standard Hindley-Milner type inference algorithm, it is not possible to support ad-hoc
polymorphism (i.e.\ overloading) without some modification.
For example, OCaml\citep{ocamlDocs} does not support ad-hoc polymorphism and instead uses, for example, the \textit{+}
operator for integer addition and the \textit{+.} operator for float addition, which is not ideal for this language 
as it would be unintuitive for a mathematician.

Examples of ML style languages that do support ad-hoc polymorphism are F\#, which uses static member functions on 
types to achieve operator overloading\citep{fsharpdocs}, and Haskell, which uses type classes\citep{haskellDocs} (constructs that define behaviour for a type, similarly to interfaces in object-oriented languages).
The way this issue was solved in Vec3 is by introducing the concept of \textit{Constraint types}, which could be 
likened to a slightly less powerful version of type classes in Haskell.
A constraint is a type that is defined by a type variable, and a function of type \textit{$Type \rightarrow bool$}.
During unification, if a type is unified with a constraint type, then the function is called with the type, and if it
returns true then the unification is successful and the type variable that the constraint holds is unified with the
type.

For example, consider the type of the \textit{+} operator:

\begin{minted}{fsharp}
(+) :: Constraint (a, supportsArithmetic) -> Constraint (a, supportsArithmetic) -> a
\end{minted}

Then, when the operator is used with say two ints, the first constrain would be unified with the type 
\textit{int} (as the int type passes the \textit{isArithmetic} function), and the type variable \textit{a} would be 
replaced with \textit{int}.
The second int would then be unified with the type \textit{int}, and the unification would be successful.
However, if the operator was used with a rational and a float, the first constraint would be unified with the type
\textit{rational}, replacing the type variable \textit{a} with \textit{rational}, and the second constraint would be 
unified with the type rational, which does not unify with float, and so the unification would fail.

This type constrain acts as a normal type, allowing for user defined ah-hoc functions, such as:

\begin{minted}{fsharp}
let double = (x) -> x + x
\end{minted}

This function can be called with any type that supports arithmetic, and the type inference algorithm will infer the
type of the function as \textit{$Constraint\ (a,\ supportsArithmetic)\ \rightarrow\ a$}.

Something else unique is the concept of a \textit{transformation}, which is a function of the constraint type that
transforms the type into another type.
This was necessary due to functions such as \textit{append}, which appends two lists.
Due to the fact that the length of the list is encoded in the type, without a transformation the arguments could 
only unify if the lists were of the same length, which is an unnecessary restriction.
As such, the transformation function is used to transform the dimensions of the first type into a list without 
dimension restrictions so unity can occur.

A current limitation of this system is that the user cannot define their own constraints, and the only constraints 
present are those built into the language (such as operators).
This is a feature that could be added in the future, but was not necessary for the current implementation of the
language.

\subsection{Vector Length Encoding}\label{subsec:vector-length-enc}

Another key feature present in the type system is the encoding of vector lengths.

The type of a vector looks like \textit{Vector of Type * Dims}, where \textit{Type} is the type of the elements, and \textit{Dims} is an integer representing the number of dimensions.
This means that a vector \textit{[1,2,3]} is inferred to be of type \textit{Vector of int * 3}.
This allows for the type system to catch errors such as adding two vectors of different lengths, or only allowing the \textit{cross product} 
function to be called on vectors of length 3.

This is a powerful feature as it allows for the type system to catch errors that would otherwise only be caught at 
runtime with standard type inference.
In its current state, it also allows for slight \textit{refinement types}\citep{freeman1991refinement}, which are types that are dependent on values, such as the length of a vector.

Examples of this catching an otherwise runtime error is shown in the following code:

\begin{minted}{fsharp}
let a = [1,2,3]
let b = [1,2]
let c = a + b // Error: Vectors must be of the same length
    
let d = [1,2,3]
let e = d[4] // Error: Index out of bounds
\end{minted}

The latter example is currently very simple, and only catches out of bound errors during indexing with constant
values, but could be extended to catch more complex errors in the future.

Another use case for this is during matrix operations, where the type system can ensure that the dimensions of the
matrices are correct, preventing errors such as finding the transpose of a non-square matrix:

\begin{minted}{fsharp}
let a = [[1,2,3], [4,5,6]]
let b = transpose(a) // Error: Matrix must be square
\end{minted}

Furthermore, the inner tensors of a matrix are also encoded with their dimensions, allowing for the type system to catch
errors such as accidentally creating a matrix with rows of different lengths:

\begin{minted}{fsharp}
let a = [[1,2,3], [4,5]] // Error: Rows must be of the same length
\end{minted}

One thing to note however is that the dimensions of a vector are lost fairly easily, for example during 
\textit{cons}, as it is not powerful enough to infer the length of the resulting vector.

Having full dependent types would solve this issue, but would be overkill for this language, and would make the
type system much more complex (likely requiring a theorem prover and types as values).

\subsection{Function Purity}\label{subsec:function-purity}

The purity of a function is also determined during type inference, with the type of a function being inferred as
pure if it is made up of only pure functions, with the base pure functions being built in.
This allows for, for example, the \textit{plotFunc} (\ref{sec:plotting}) ensuring that only pure functions can be 
passed to it, preventing a user from passing a function that has side effects (which would likely cause a runtime 
error otherwise).
It also allows for easy dead code elimination, as a call to a function that has no side effects can be removed if the
result is not used.
