\section{Final BNF}\label{sec:final-bnf}

\input{final-bnf}

\section{Final GUI}\label{sec:final-gui}

\section{Notable Features}\label{sec:notable-features}

\section{Lexer}\label{sec:lexer}

Initial lexer design was based on a simple regular expression based lexer, but this was later replaced with a more
functional approach using pattern matching on the input string.

The reason for this change was that the regular expression based lexer was difficult to extend and maintain due to 
the lack of type safety.
For example if we had a more general regex called before a more specific one, the more general one would always match
first, even if the more specific one should have matched.

This was solved by using a more functional approach, where the type system of F\# would inform us if a case would 
never be matched due to the order of the cases or otherwise, preventing a class of easily overlooked errors during development.

The lexer is now implemented as a recursive pattern matching function that takes a string and returns a list of 
tokens, complete with their lexeme and position in the input string.

Lexer errors are also accumulated in a list of type \textit{LexerError}, which are displayed to the user in the GUI\@.

Something of note is that the lexer parses numbers itself, rather than passing them to the parser as strings.

Additionally, due to the permittance of user defined operators, the lexer makes special considerations when lexing 
special characters, as the distinction between a built-in operator (with precedence) and a user defined operator (
currently without taking precedence into account) is made during lexing.

Furthermore, both block comments (\textit{/* */}) and line comments (\textit{//}) are handled by the lexer by ignoring
the contents of the comment.
In future, it may be interesting represent comments as a token in the AST, allowing for systems such as documentation
generation or automatic formatting to be implemented.

\section{Parser}\label{sec:parser}

The parser is implemented using Pratt parsing\citep{pratt1973top}, which is a top-down operator precedence parsing 
method that allows for easy extension and modification of the grammar.

It works by assigning a precedence to each token, as well as functions specifying how to parse the token when 
encountering it in a prefix, infix or postfix position.

For example, take the expression $2 + 3 * 4$.

The parser would first encounter the number \textit{2}, which has a precedence of 0 and a prefix function that
simply returns the number.

Thus, the current state of the parser is $2$.

The parser would then encounter the operator \textit{+}, which has a precedence of 1 and a left associative infix
function that takes the left hand side and the right hand side and returns a binary expression node.

The parser then attempts to parse the right hand side of the operator with a precendence level higher than the
plus operator, as Pratt parsing must ensure that higher precedence operations (such as multiplication) are parsed
first.

The parser would then encounter the number \textit{3}, which again is treated as a literal and returned.

The parser then encounters the operator \textit{*}, which has a precedence of 2 (higher than the plus operator) and 
as such the parser cannot yet resolve the \textit{+} operator; it must handle the higher precedence multiplication
operator first.

The parser saves the left hand side (the number 3) and then parses the right hand side of the multiplication 
operator using a precedence level higher than the multiplication operator.

It encounters the number \textit{4}, which is returned as a literal.

The parser then returns the binary expression node for the multiplication operator, with the left hand side being
the number 3 and the right hand side being the number 4.

The parser then returns to the plus operator, which can now be resolved as the left hand side is the number 2 and the
right hand side is the result of the multiplication operator.

This is a simple example, but Pratt parsing can handle more complex expressions with ease, such as nested
expressions and function calls.

Using Pratt parsing has improved the extensibility of the parser, as adding new operators or changing the grammar
is as simple as adding a new case to the parser.

A slight limitation is during ambiguity, such as the \textit{(} symbol, which can be used for a grouping, a lambda 
definition, a tuple or a \textit{unit} type when encountered in the prefix position.
This is resolved through a state machine approach, where the parser can move around the state at will, allowing 
lookahead and backtracking in order to reach a point where the ambiguity is resolved.

In order to simplify the Virtual Machine\ref{sec:virtual-machine}, the parser parses all binary and unary operations 
as function calls, with the operator as the function name.

In order to make type inference simpler for operators that are overloaded for both unary and binary operations (such 
as the \textit{-} operator), the operator itself keeps track of the manner in which it is called (unary or binary) and
returns the appropriate AST node. 
This allows for easier type inference (as the names of the overloaded functions are different), 
and simplifies the bytecode generation process by removing ambiguity in the AST\@.

This idea could possibly be extended to allow other overloaded function names (with varying numbers of arguments or 
arguments of different types).

\section{AST}\label{sec:expression}

The AST of the language is represented as a list of statements, where a statement is either expression, a
variable assignment or an other statement type.
It is typed (after type inference\ref{sec:type-inference}) in order to allow for easier optimisation and
bytecode generation.

The AST representation is given in section~\ref{sec:final-bnf}.

\section{Type Inference}\label{sec:type-inference}
\input{type-inference}

\section{Optimisation}\label{sec:optimisation}

Before compilation, the AST is optimised by removing dead code and constant folding.

\subsection{Dead code elimination}\label{subsec:dead-code-elimination}

Dead code elimination is performed on the AST by removing any statements that are not used.
For example, if a variable is declared but never used, the variable declaration is removed or if an expression is
written but never used, the expression is removed.

This is accomplished by through static analysis of the AST, where the following process is repeated until no more dead
code can be removed:

\begin{algorithmic}
    \While{Dead code can be removed}
        \For{Each node in the AST}
            \If{Node is a statement}
                \If{Statement is not used}
                    \State Remove statement
                \EndIf
            \ElsIf{Node is an expression}
                \If{Expression is not used}
                    \State Remove expression
                \EndIf
            \ElsIf{Node is a binding}
                \If{Variable is not used}
                    \State Remove binding
                \EndIf
            \EndIf
        \EndFor
    \EndWhile
\end{algorithmic}

The process is repeated until no more dead code can be removed, allowing for long chains of dead code to be 
removed (for example if a variable is used in a function that is never called, the function would first be removed
and then the variable).
It is to be noted that variable assignments are never removed during DCE when running the code editor due to the 
attached REPL, as the user may wish to use the variable in the REPL\@, or when running code blocks in the notebook 
view\ref{sec:notebook-view} as the variable may be used in a later code block.
However, DCE can be aggressively performed when transpiling to C\ref{sec:transpiler}, as the user is not expected to 
interact with the generated C code.

\subsection{Constant folding}\label{subsec:constant-folding}

Constant folding is performed on the AST by evaluating constant expressions at compile time, such as $2 + 2 \ra 4$.
This is accomplished using the initial interpreter implementation, which recursively evaluates the AST and replaces
constant expressions with their evaluated value.
Only constants are evaluated, and thus no variable resolution is performed due to the cost of this operation.

\section{Initial Design of the Bytecode Virtual Machine and Compiler}\label{sec:initial-design-of-the-bytecode-virtual-machine-and-compiler}

The core of the Vec3 interpreter was a transition from a tree-walk interpreter to a more efficient stack-based virtual machine. 
This involved two primary components: a compiler to translate Vec3 source code into bytecode, and a virtual machine to execute that bytecode. 
The fundamental goal was to achieve faster execution speeds by working with a compact and streamlined instruction set.

The bytecode itself was a sequence of instructions, each represented by an operation code (opcode) and, potentially, operands that the opcode would act upon. 
These opcodes, defined in the \texttt{OP\_CODE} type, covered a range of operations necessary for a fully functional language. 
This included instructions for pushing constants onto the stack (\texttt{CONSTANT}, \texttt{CONSTANT\_LONG}), performing arithmetic (\texttt{ADD}, \texttt{SUBTRACT}, \texttt{MULTIPLY}, \texttt{DIVIDE}, \texttt{NEGATE}), managing control flow (\texttt{RETURN}, \texttt{JUMP}, \texttt{JUMP\_IF\_FALSE}, \texttt{LOOP}), handling boolean logic (\texttt{NIL}, \texttt{TRUE}, \texttt{FALSE}, \texttt{NOT}, \texttt{EQUAL}, \texttt{GREATER}, \texttt{LESS}), manipulating the stack (\texttt{POP}), working with global and local variables (\texttt{DEFINE\_GLOBAL}, \texttt{GET\_GLOBAL}, \texttt{SET\_GLOBAL}, \texttt{GET\_LOCAL}, \texttt{SET\_LOCAL}), outputting values (\texttt{PRINT}), and calling functions (\texttt{CALL}). Two functions, \texttt{opCodeToByte} and \texttt{byteToOpCode}, handled the conversion between these symbolic opcodes and their corresponding byte representations, ensuring a compact bytecode format.

Compiled code, along with associated data, was organized into \texttt{Chunk} structures. 
Each chunk contained a \texttt{Code} array, holding the bytecode instructions as a sequence of bytes. 
A \texttt{ConstantPool} array stored constant values referenced by the instructions, enabling efficient reuse of values like numbers and strings. 
To aid in debugging, a \texttt{Lines} array mapped bytecode offsets to their corresponding line numbers in the original source code. 
Functions like \texttt{emptyChunk}, \texttt{writeChunk}, \texttt{addConstant}, \texttt{writeConstant}, and \texttt{getLineNumber} provided an interface for creating and manipulating chunks.

The compiler's role was to transform the abstract syntax tree (AST) representation of Vec3 code into this bytecode format. 
It maintained a \texttt{CompilerState} to track the chunk being generated, local variables within the current scope, the current scope's nesting depth, and the line number being processed. 
The compilation process involved a recursive descent through the AST. Functions like \texttt{compileStmt} and \texttt{compileExpr} recursively processed statements and expressions, respectively. 
For each AST node encountered, the compiler emitted corresponding bytecode instructions using helper functions like \texttt{emitByte}, \texttt{emitBytes}, \texttt{emitConstant}, and \texttt{emitOpCode}.

Variable declarations were handled by \texttt{compileVariableDeclaration}, which added the variable to the \texttt{Locals} map in the \texttt{CompilerState}. 
This map stored local variables and their corresponding slot indices on the virtual machine's stack. 
Control flow instructions, like jumps and loops, were initially emitted with placeholder offsets. 
These placeholders were later patched with the correct offsets once the target locations were determined.

\textbf{Error Handling}

Error handling during compilation was managed using the \texttt{CompilerResult} type. 
This allowed the compiler to either return a successful result along with an updated \texttt{CompilerState} or an error along with a descriptive message and the state at the point of the error.

\textbf{Debugging}

To facilitate debugging, a disassembler was implemented. 
Functions like \texttt{disassembleInstruction} and \texttt{disassembleChunk} took the compiled bytecode and produced a human-readable representation, showing the instructions, their operands, and their associated source code line numbers. 
This was invaluable for understanding the generated bytecode and identifying potential issues.

\section{Virtual Machine}\label{sec:virtual-machine2}

The Virtual Machine (VM) was responsible for executing the compiled bytecode. 
It was designed as a stack-based machine, meaning that it used a stack to store intermediate values during computation. 
The VM's state was represented by the \texttt{VM} type, defined as follows:

\begin{minted}{fsharp}
type VM = {
    Chunk: Chunk
    IP: int
    Stack: ResizeArray<Value>
    ScopeDepth: int Â 
}
\end{minted}

\noindent where:

*   \texttt{Chunk} held the bytecode and associated data (constant pool, line information) currently being executed.
*   \texttt{IP} (Instruction Pointer) was an integer representing the index of the next bytecode instruction to be executed.
*   \texttt{Stack} was a dynamically sized array used to store values during computation. Operations like arithmetic, comparisons, and function calls would push and pop values from this stack.
*   \texttt{ScopeDepth} tracked the current level of scope nesting.

The \texttt{createVM} function initialized a new VM instance with a given chunk. The core of the VM was the \texttt{run} function, a recursive loop that fetched, decoded, and executed instructions until a \texttt{RETURN} instruction was encountered or an error occurred.

Key helper functions included:

*   \texttt{push}: Pushed a value onto the stack.
*   \texttt{pop}: Popped a value from the stack.
*   \texttt{peek}: Looked at a value on the stack at a given position without removing it.
*   \texttt{readByte}: Read the byte at the current instruction pointer and incremented the IP.
*   \texttt{readConstant}: Read a constant index from the bytecode, fetched the corresponding value from the constant pool, and pushed it onto the stack.
*   \texttt{readConstantLong}: Similar to \texttt{readConstant}, but for constants that required a larger index.
*   \texttt{binaryOp}: Performed a binary operation on the top two values on the stack.

The \texttt{run} function used a match expression to dispatch to the appropriate code based on the current opcode. Each opcode case handled a specific instruction, potentially manipulating the stack, performing calculations, or managing control flow.

The \texttt{interpret} function provided the main entry point for executing a chunk of bytecode. It first disassembled the chunk for debugging purposes, then created a new VM instance and called \texttt{run} to start the execution process.

In conclusion, this initial design established a solid foundation for a stack-based virtual machine and its associated compiler. It emphasized a clean separation of concerns between compiling and executing code, a compact bytecode representation, and a focus on essential features for a functional language. The inclusion of debugging tools, a well-defined error-handling mechanism, and a dedicated virtual machine for execution further contributed to the robustness and efficiency of the system.

\section{Virtual Machine}\label{sec:virtual-machine}

\section{Prelude}\label{sec:prelude}

A prelude is implicitly included in every program, which contains some useful functions defined in the language, as 
well as wrappers for the built-in functions of the Virtual Machine. Initially, defining the following instructions:

Notable functions include:

\begin{itemize}
    \item \textit{map}, \textit{fold} and \textit{filter} functions for lists.
    \item \textit{range} function for generating a list of numbers.
    \item \textit{sqrt}, \textit{cubeRoot} which are specialisations of the \textit{root} function.
    \item \textit{head}, \textit{tail} and \textit{len} functions for lists.
    \item \textit{findIntegral} function for finding the integral of a function.
\end{itemize}

We felt it was useful implementing these in-language functions as it allows for more concise and readable code, as
well as showcasing the power of the language.

\section{Plotting}\label{sec:plotting}

The plotting system is implemented using ScottPlot\citep{scottPlot}, a plotting library for .NET\@.

The functionality is exposed to the user through 3 built-in functions: \textit{plot}, \textit{plotFunc} and 
\textit{plotFuncs}.

\textit{plot} takes in a record of configuration options of the following type:

\begin{minted}{fsharp}
    type PlotConfig = {
        title: string,
        XValues: [float],
        YValues: [float],
        ptype: "bar" | "scatter" | "signal",
    }
\end{minted}

The resulting plot is then displayed in a separate window based on these configuration options.

\todo{Add images of plots}

The \textit{plotFunc} function takes in a string title and a pure function of type \textit{float -> float}.
The function is then plotted on the graph with an infinite range of x values.
Optionally, the user can also specify two more float values, \textit{start} and \textit{end}, in which case the 
integral of the function is calculated and displayed on the graph.

\todo{Add images of plots}

The \textit{plotFuncs} function takes in a string title and a list of pure functions of type \textit{float -> float}.
This allows for multiple plots to be placed on the same window, which we felt was valuable for comparing functions 
or plotting derivatives.

The plot windows also have an input at the bottom, which allows for the user to input a function and have it plotted
on command.
This is useful for quick visualisation of functions, and allows for a more interactive experience.

\section{Drawing}\label{sec:drawing}

As well as plotting, the user also has the option of drawing arbitrary shapes on a canvas, and attaching event 
listeners to them.

This is done by means of the \textit{draw} function, which takes in a record of configuration options of the following
type:

\begin{minted}{fsharp}
    type DrawConfig = {
        x: float,
        y: float,
        width: float,
        height: float,
        color: string,
        shape: "rectangle" | "circle",
        trace?: bool, 
    }
\end{minted}

Or a list of the above record type, allowing for multiple shapes to be drawn on the same canvas.

The \textit{draw} function then returns a unique identifier for the shape, which can be used to attach event 
listeners, allowing for movement of the shape through key presses.

The following example attached event listeners to a shape which moves it left and right following the \textit{cos} curve:

\begin{minted}{fsharp}
on(id, Keys.Right, (state) -> { x = state.x + 10.0, y = cos(state.x) * 10.0 + 100.0 })
on(id, Keys.Left, (state) -> { x = state.x - 10.0, y = cos(state.x) * 10.0 + 100.0 })
\end{minted}

Where keys is a record defined in the prelude of the language (see~\autoref{sec:prelude}).

Additionally, the \textit{trace} option allows for the shape to leave a trail behind it, which can be useful for
animations or visualising movement.

\todo{Add images of drawings}

\section{Notebook View}\label{sec:notebook-view}

The notebook view is a feature that allows the user to write code in a more interactive way, similar to Jupyter
notebooks\citep{Jupyter}.

\section{Transpiler}\label{sec:transpiler}

The user also has the option of transpiling their code to C, which can then be compiled and run as a standalone
executable, allowing for faster execution of the code which is important for larger or more computationally
intensive programs.

\section{Code architecture}\label{sec:code-architecture}